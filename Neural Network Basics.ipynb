{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Neural Network Basics\n",
    "\n",
    "**Kyle Caverly - June 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A few helpful resources related to Neural Networks along with additional notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building Blocks of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Concepts and Models: Artificial Neural Nets](https://missinglink.ai/guides/neural-network-concepts/complete-guide-artificial-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Extensive article that covers all most basic elements of a Neural Network, including:\n",
    "\n",
    "* Neurons/Perceptrons, Layers, Weights and Activations\n",
    "* Backpropagation\n",
    "* Activation Functions\n",
    "* Bias Neutrons\n",
    "* Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [7 Types of Activation Functions](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Helpful overview of the primary activation functions, and the importance of using a non-linear activation layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Visualizing Activation Functions](https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Interactive web tool, visualizing both the activation functions and their derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Understanding Softmax & Negative Log Likelihood Loss](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Simple visualization exploring the function of Softmax probabilities and the negative log likelihood loss calculations, primarily used in multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Kullback-Leibler Divergence Explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Kullback-Leibler (KL) Divergence measures the amount of information we lose when we choose an approximation for a prior distribution. It is frequently used to compare prior/posterior probability distributions in models like Variational Auto Encoders.\n",
    "* KL Divergence is heavily related to the idea of information entropy. We can interpret information entropy as \"the minimum number of bits it would take us to encode our information.\" Entropy ultimately provides us with a way to measure the information available in our original distribution. \n",
    "* KL Divergence is simply a measure that measures the expected information loss between a prior known distribution with a approximating distribution. It is the difference in entropy between these two distribution.\n",
    "* KL Divergence is not a distance metric, as KL Divergence is not symettric, if you change which distribution is the prior and which is the approximated distribution, you will end up with different KL Divergence metrics.\n",
    "* Optimizing for KL Divergence: You may notice that KL Divergence measures the exact loss we commonly compare when performing classification tasks using a softmax function. However, with Softmax functions, it is most common to optimize using \"Cross-Entropy\" Loss, not KL Divergence. This is not because KL Divergence is not the appropriate metric in this use case. Mathetmatically, KL Divergence is simply the Cross Entropy minus the Original Entropy. As the Original Entropy is a scalar value, its derivative is simply 1. As such, optimizing for KL Divergence is mathematically the same as optimizing for Cross-Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Overview of Gradient Descent Optimization Algorithms](https://ruder.io/optimizing-gradient-descent/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Great explanation specifically surrounding SGD and the challenges associated with different Gradient Descent variants.\n",
    "\n",
    "Overview of the Three Primary Gradient Descent Variants:\n",
    "1. Batch Gradient Descent\n",
    "2. Stochastic Gradient Descent\n",
    "3. Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Visualization of ANN Training**](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Really helpful article that sums up what is going on behind the scenes during Neural Network training really well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Training & Practical Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Original Paper detailing dropout, and its application and valud-add wrt regularization.\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. This means that their contribution to the activation of downstream neurons is temporarily removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "Practically - Dropout is used primarily to regularize on relatively noisy datasets, as it makes it more difficult for the model to learn, pushing it away from common heuristics as a crutch.\n",
    "\n",
    "Notably, Dropout is not used on some newer SOTA NLP models, as the size of the datasets, removes the ability for any model to reliably overfit - thus regularization is often redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Original paper detailing Batch Normalization, the uptick in training and convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Vanishing Gradients](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A brief article discussing the problem of vanishing gradients in deeper neural networks or RNN architectures in which the gradient slowly shrinks to near 0, as the subseqent layer derivatives are multiplied together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Exploding Gradients](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A brief article discussing the problem of exploding gradients in deeper neural networks or RNN architectures in which the gradient accumulates through the layers, if the higher layer gradients are above 1. This often leads to unstable networks in which the network is unable to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Learning From Noisy Labels with Deep Neural Networks: A Survey](http://arxiv.org/pdf/2007.08199v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Discuss strategies to enhance and learn from noisy labels. Given the occurence of noisy labels in practice with neural networks, argues Robust NN training is becoming only a more important topic.\n",
    "* The ratio of corrupted labels in real-world datasets is reported to range from 8.05% to 38.5%. As DNNs often have model parameters than samples, it is possible for DNNs to memorize noise in datasets, unfortunately heavily decreasing generalization.\n",
    "* Additionally, the accuracy drop with label noise is considered to be more harmful than with other noises, such as feature noise.\n",
    "* Recently, model architectures have been built upon to specifically identify and explore the noise in the original dataset. An example, is adversarial training enhances the noice tolerance by encouraging the DNN correctly to classify both original input and hostilely perturbed ones.\n",
    "* They do mention that noise robustness is significantly improved using several semi-supervised techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Transfer Learning and Multi-Task Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Overcoming Catastrophic Forgetting in Neural Networks](https://arxiv.org/pdf/1612.00796.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Discusses the ability for neural networks to learn tasks sequentially without forgetting or reduced performance on previous tasks. This has been widely thought of as an inenviable feature of connectionist models. They argue that it is possible to overcome this limitation and maintain expertise on tasks  which have not been trained on for a long time.\n",
    "* Critically, intelligent agents must demonstrate a capacity for continual leaning: that is, the ability to learn consecutive tasks without forgetting how to perform previously trained tasks. While this is immediately applicable in areas like reinforcement learning, it is incredibly applicable for both transfer learning and multi-task learning generally.\n",
    "* Catastrophic forgetting is the tendency for knowledge of previously learnt task(s) to be abruptly lost as information relevant to the current task is incorporated. This phenomenon, termed **catastrophic forgetting** occurs specifically when the network is trained sequentially on multiple tasks because the weights in the network that are important for task A are changed to meet the objectives of task B.\n",
    "* Current approaches have typically ensured that data for all tasks are simultaneously available during training. In which the weights of the network is jointly optimized for performance on all tasks, in a multi-task learning setting.\n",
    "* They discuss a method in which the previously learnt weights are strengthened, and stiff to move as a new task is learned, hopefully maintaining the previous learning while adapting to a new task. Ultimately however, it comes down to the capacity of an individual network. Learning too many tasks will lead to a decrease in performance across all tasks. \n",
    "* For context, as of 2020 Google has found that their Machine Translation algorithms can handle 8 language pairs at a specific time, trained inside a multi-task setting before seeing sharp decreases in accuracy across all tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [A Survey on Deep Transfer Learning](https://arxiv.org/pdf/1808.01974.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\"**Transfer learning** relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data.\"\n",
    "* \"Data dependence is one of the most serious problem in deep learning. Deep learning has a very strong dependence on massive training data compared to traditional machine learning methods, because it needs a large amount of data to understand the latent patterns of data.\"\n",
    "* In transfer learning, the training data and test data are not required to be iid and the model in target domain is not needed to be trained from scratch, which can significantly reduce the demand of training data and training time in the target domain.\n",
    "* Instances-based deep transfer learning refers to use a specific weight adjustment strategy, select partial instances from the source domain as supplements to the training set in the target domain by assigning appropriate weight values to these selected instances.\n",
    "* Mapping-based deep transfer learning refers to mapping instances from the source domain and target domain into a new data space. In this new data space, instances from two domains are similarly and suitable for a union deep neural network.\n",
    "* Network-based deep transfer learning refers to the reuse of the partial network that is pre-trained in the source domain, including its network structure and connection parameters, transfer it to be a part of a deep neural network which is used in the target domain. This is the common model in NLP processing, and the reason for successful deep learning based models like BERT etc. \n",
    "* Adversarial-based deep transfer learning refers to introduce adversarial technology inspired by generative adversarial nets (GAN) to find transferable representations that are applicable to both the source domain and the target domain. It is based on the assumption that \"For effective transfer, good representation should be discriminate for the main learning task and indiscriminate between the source domain and target domain.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Generalization and Model Capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Understanding Deep Learning Requires Rethinking Generalization](https://arxiv.org/pdf/1611.03530.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice.\"\n",
    "* \"Deep neural networks easily fit random labels. More precisely when trained on a completely random labeling of the true data, neural networks achieve 0 training error. In such a scenario, learning is impossible. Thus the effective capacity of neural networks is sufficient for memorizing the entire data set.\" Futhermore, when trained on random labels, \"several properties of the training process for multiple standard architectures is largely unaffected by this transformation of the labels.\" As in, training was not substantially slowed, and still converged, despite no learnable structure in the data.\n",
    "* \"Explicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error. In contrast with classical convex empirical risk minimization, where explicit regularization is necessary to rule out trivial solutions, we found that regularization plays a rather different role in deep learning. It appears to be more of a tuning paramtere that often helps improve the final test error of a model, but the absence of all regularization does not necessarily imply poor generalization error.\"\n",
    "* When explicit regularization techniques like dropout and weight decay, common models are still able to fit random training data extremely well if not perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/abs/1912.02292) [(Blog)](https://openai.com/blog/deep-double-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\"We show that a variety of modern deep learning tasks exhibit a \"double-descent\" phenomenon where, as we increase model size, performance first gets worse and then gets better.\" Heavily relates to the interpolation threshold, overfitting, and model generalization.\n",
    "\n",
    "* Discusses the **bias-variance** tradeoff in traditional statistical learning as \"The idea is that models of higher complexity have lower bias but higher variance. According to this theory, once model complexity passes a certain threshold, models \"overfit\" with the variance term dominating the test error, and hence from this point onward, increasing model complexity will only decrease performance (ie. increase test error). Hence conventional wisdom in classical statistics is that, once we pass a certain threshold, \"larger models are worse.\". However, modern neural networks exhibit no such phenomenon, which conventional wisdom among practioners is that \"larger models are better\".\n",
    "* Show that many deep learning settings have two different regimes. In the under-parameterized regime, where the model complexity is small compared to the number of samples, the test error as a function of model complexity follows the U-Like behaviour predicted by the classical bias/variance tradeoff. However, once model complexity is sufficiently large to interpolate (ie. achieve close to zero training error) then increasing complexity only decreases test error, following the modern intuition of \"bigger models are better\".\n",
    "* Models with more noise often show even greater effects of the Double Descent phenomenon, making the importance of model size even greater. As it can be difficul to identify if the model has overfit or is experiencing double descent with smaller noisy models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Stiffness: A New Perspective on Generalization in Neural Networks](https://arxiv.org/pdf/1901.09491.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Develop a perspective on generalization of neural networks by proposing and investigating the concept of neural network stiffness. We measure how stiff a network is by looking at how a small gradient step in the networks parameters on one example affects the loss on another example.\n",
    "* Stiffness captures the resistance of the functional approximation learned to deformation by gradient steps.\n",
    "* As training begins, the model is often seen as \"stiff\" within classes. Ie. A gradient change to a class benefits only the members of the same class. However, as the model approaches overfitting, the model becomes less stiff, as gradient changes from one example does not lead to a consistent improvement on other examples within the same class. This is reasonable, as with overfitting, more detailed specific features have to be learned, which may not truly be apparent throughout the entire class.\n",
    "* They argue that when trained on CIFAR10, stiffness is noticably higher for the hierarchial super-classes present, which reveals that the network is aware of higher-order semantically meaningful categories to which the images belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Do CIFAR-10 Classifiers Generalize to CIFAR-10](https://arxiv.org/pdf/1806.00451.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Argues that the impressive accuracy numbers of the best performing models are questionable because the same test sets have been used to select these models for years now. They look to understand the danger of overfitting on test data. While the majority of the paper explores this from a Computer Vision perspective, the ideas broadly are applicable across a variety of problem spaces.\n",
    "* \"Properly evaluating progress in machine learning is subtle. After all, the goal of a learning algorithm is to produce a model that generalizes well to unseen data. Since we usually do not have access to the ground truth data distribution, we instead evaluate a model's performance on a seperate test set. This is indeed a principled evaluate protocol, as long as we do not use the test set to select our models. Unfortunately, we typically have limited access to new data from the same distribution. It is now commonly accepted to re-use the same test set multiple times throughout the algorithm and model design process.\"\n",
    "* To explore the result of overfitting on a specific test dataset, they test pretrained CIFAR-10 models, on a truly new unseen set of image data which has only a minute distributional shift. They find a large drop in accuracy (4% to 10%), ultimately identifying that slightly distributional shifts are a large area of concern for true generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Confidence, Trust and Explainability in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Probability vs. Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [The Importance of What We Dont Know](http://mlg.eng.cam.ac.uk/yarin/thesis/1_introduction.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Deep Learning Models are generally seen as deterministic functions, as such we often only have point estimates of parameters and predictions at hand. This thesis, discusses tools which can be leveraged to get a better understanding of uncertainty in deep learning models.\n",
    "\n",
    "* Define **model expressiveness** as \"the complexity of functions a model can capture\"\n",
    "* Intuitively explain out of distribution test data: \"For example, given several pictures of dog breeds as training data -- when a user uploads a photo of his dog -- the hypothetical website should return a prediction with rather high confidence. But what should happen if a user uploads a photo of a cat and asks the website to decide on a dog breed? The above is an example of out of distribution test data.\"\n",
    "* Given an out of distribution use case, A possible desired behaviour of a model in such cases would be to return a prediction (attempting to extrapolate far away from our observed data), but return an answer with the added information that the point lies outside of the data distribution. As such, we want our model to possess some quantity conveying a high level of uncertainty with such inputs (alternatively, conveying low confidence).\n",
    "* Discuss how in classificaion models, the probabilty vector obtained at the end of the pipeline (the softmax output) is often erroneously interpreted as model confidence. As a model can be uncertain in its predictions even with a high softmax output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Paper discussing confidence calibration (the gap between probability and model accuracy), and how different training strategies and post training calibration methods like temperature scaling help.\n",
    "* Specifically, a network should provide a calibrated confidence measure in addition to its prediction. In other words, the probability associated with the predicted class label should reflect its ground trust correctness likelihood.\"\n",
    "* \"While neural networks today are undoubtably more accurate than they were a decade ago, we discover with great surprise that modern neural networks are no longer well-calibrated.\"\n",
    "* They propose ECE, as a measure of calibration, by binning predictions into equally spaced bins, and taking a weighted average of the bins' accuracy/confidence difference.\n",
    "* \"Although increasing depty and width may reduce classification error, we observe that these increases negatively affect model calibration... ECE Metric grows substantially with model capacity.\"\n",
    "* \"We do observe that models trained wth Batch Normalization tend to be more miscalibrated.\"\n",
    "* \"Model calibration continues to improve when more regularization is added, well after the point of achieving optimal accuracy.\"\n",
    "* Discuss the disconnect between NLL and accuracy, which occurs because neural networks can overfit to NLL without overfitting to the 0/1 loss This phenomenon renders a concrete explanation of miscalibration: the network learns better classification accuracy at the expense of well-modeled probabilities. Argue that overfitting may manifest itself in probabilistic error rather than classification error.\n",
    "* \"Our most important discovery is the surprising efectiveness of temperature scaling despite its remarkable simplicity. Temperature scaling outperforms all other methods on vision tasks, and performs comparably to other methods on the NLP dataset.\"\n",
    "* Temperature scaling is a post-training process, in which a variable T, is learned minimizing for NLL on a validation set, which \"softens\" the softmax, without ultimately affecting model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Evidential Deep Learning to Quantify Classification Uncertainty](https://arxiv.org/pdf/1806.01768.pdf) ([Blog](https://towardsdatascience.com/softmax-and-uncertainty-c8450ea7e064))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Discussion on drawbacks of Softmax as a means of producing a probability vector. Propose a method slightly augmenting the loss function to form opinions which withold classification if they are not confident.\n",
    "* Outlines how with basic data augmentation, a classifier on MNIST data can have 100% confidence for an incorrect guess.\n",
    "* Discusses how a neural network is capable of forming opinions for classification tasks. Thus quantifying uncertainty surrounding multi-class prediction.\n",
    "* They argue that such a method, outperforms on out-of-distribution sample data, and is more protected from adversarial attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Explainability & Trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [LIME: Locally Interpretable Model-Agnostic Explanations](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Discussion of LIME, a leading technique used to describe the predictions of any machine learning classifier. Leading work, focused on developing and interacting with an approximation of black box networks locally around a specific prediction, to explore the model and build trust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A comprehensive resource discussing Shap values, commonly used to identify feature importance and interpretability in ml models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Pruning and Lottery Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [What's Hidden in a Randomly Weighted Neural Network?](https://arxiv.org/abs/1911.13299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Interesting work exploring successful sub-networks found in a random initialization of a neural network.\n",
    "\n",
    "\"Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [The Lottery Ticket Hypothesis: Finding, Sparse Trainable Neural Networks](https://arxiv.org/pdf/1803.03635.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Really interesting work exploration the selection of sub-networks inside Neural Networks. \n",
    "\n",
    "Really interesting work exploring model pruning, initialization and network substructures. Found that if you randomly initialize a network, train to convergence, prune, and \"rewind training\" by recreating the pruned sub-network with the randomly initiated weights, and retraining, it is possible to match or surpass accuracy. Further supporting the general consensus that neural networks are aggressively overparameterized with model performance still dependent on intialization to a certain extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [The Lottery Ticket Hypothesis: A Survey](https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "An outstanding summary article walking through developments related to the Lottery Ticket Hypothesis and weight pruning in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Neural Networks in Unsupervised/Semi-Supervised Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [An Introduction to Variational Auto-Encoders](https://arxiv.org/pdf/1906.02691.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\"Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational automencoders and some important extensions.\"\n",
    "* \"One major division in machine learning is generative versus discriminative modeling. While in discriminative modeling one aims to learn a predictor given the observations, in generative modeling one aims to solve the more general problem of learning a joint distribution over all the variable. A generative model simulates how the data is generated in the real world.\"\n",
    "* \"Generative modeling can be useful more generally. One can think of it as an auxiliary task. For instance, predicting the immediate future may help us build useful abstractions of the world that can be used for multiple prediction tasks downstream. This quest for disentangled, semantically meaningful, statistically independent and causal factors of variation in data is generally known as unsupervised representation learning, and the variational autoencoder (VAE) has been extensively employed for that purpose.\"\n",
    "* \"The VAE can be viewed as two coupled, but independently parameterized models: the encoder or recognition model, and the decoder or generative model. These two models support each other. The recognition model delivers to the generative model an approximation to its posterior over latent random variables, which it needs to update its parameters inside an iteration of \"expectation maximization\" learning. Reversely, the generative model is a scaffolding of sorts for the recognition model to learn meaningful representations of the data, including possible class-labels. The recognition model is the approximate inverse of the generative model according to Bayes rule.\"\n",
    "* \"The most common criterion for probabilistic models (including VAE) is maximum log-likelihood. As we will explain, maximization of the log-likelihood criterion is equivalent to minimization of a Kullback Leibler divergence between the observed data and model distributions.\"\n",
    "* Discuss KL Annealing, and mode collapse: \"In our work, we found that stochastic optimization with the unmodified lower bounce objective can get stuck in an undesirable stable equilibrium. At the start of training, the likelihood term log p(x|z) is relatively weak, such that an intitially attractive state is ehere q(z|x) = p(z), resulting in a stable equilibrium from which it is difficult to escape. The solution proposed is to use an optimization schedule where the weights of the latent cost Dkl is slowly annealed from 0 to 1 over many epochs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### [Balancing Reconstruction Error and Kullback-Leibler Divergence in Variational Autoencoders](https://arxiv.org/pdf/2002.07514.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\"In the loss function of a Variational Autoencoder there is a well known tension between two components: the reconstruction loss, improving the quality of the resulting images, and the Kullback-Leibler divergence, acting as a regularizer of the latent space.\" This paper explores this balance and explores solutions.\n",
    "* The loss function of VAEs are composed of two parts: one is just the log-likelihood of the reconstruction, while the second one is a term aimed to enforce a known prior distribution of the latent space - typically a specifical normal distribution. Technically, this is achieved by minimizing the KL Divergence.\n",
    "* Loglikelihood and KL-Divergence are typically balanced by a suitable beta parameter, since they have somewhat contrasting effects; the former will try to improve the quality of the reconstruction, neglecting the shape of the latent space; on the other side, KL-Divergence is normalizing and smoothing the latent space, possibly at the cost of some additional \"overlapping\" between latent variables, eventually resulting in a more noisy encoding.\n",
    "* If not properly tuned, KL-Divergence can also easily induce a sub-optimal use of network capacity, where only a limited number of latent variables are exploited for generation; this is the so called overpruning/variable-collapse/sarsity phenomenon.\n",
    "* Tuning down beta typically reduces the number of collapsed varables and improves the quality of reconstructed sample. However, this may not result in a better quality of generated samples, since we loose control on the shape of the latent space, that becomes harder to be exploited by a random generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing](https://arxiv.org/abs/1903.10145)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [Diagnosing and Enhancing VAE Models](https://arxiv.org/abs/1903.05789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation](https://www.aclweb.org/anthology/D19-5612.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Variational Autoencoders (VAEs) are known to suffer from learning uninformative latent representation of the input due to isues such as approximated posterior collapse, or entanglement of the latent space. We impose an explicit contraint on the KL Divergence term inside the VAE objective function. While the explicit constraint naturally avoids posterior collapse, we use it to further understand the significance of the KL term in controlling the information transmitted through the VAE channel. Within this framework, we explore different properties of the estimated posterior distribution, and highlight the trade-off between the amount of information encoded in a latent code during training, and the generative capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
