{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Statistical Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [A User's Guide to Support Vector Machines](http://pyml.sourceforge.net/doc/howto.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Support Vector Machine (SVM) is a widely used classifier. We describe the effect of the SVM parameters on the resulting classifier, how to select good values for those parameters, data normalization, factors that affect training time.\n",
    "* SVMs belong to the general category of kernel methods. A kernel method is an algorithm that depends on the data only through dot-products. When this is the case, the dot product can be replaced by a kernel function which computes a dot product in some possibly high dimensional feature space. This has two advantages: First, the ability to generate non-linear decision boundaries using methods designed for linear classifiers. Second, the use of kernel funcions allows the user to apply a classifier to data that have no obvious fixed-dimensional vector space representation.\n",
    "* The basic support vector machine is a linear two-class classifier. In which a linear decision boundary defined by a hyperplane classifies the input space into positive/negative classes.\n",
    "* Linear SVMs can be extended by applying non-linear mapping functions, to generate non-linear decision boundaries. However, the approach of explicitly computing non-linear features via a specifically created non-linear mapping of the features, does not scale well with the number of input features. Kernel methods solve this issue by avoiding the step of explicily mapping the data to a high dimensional feature-space.\n",
    "* Traditionally, SVM's are solved as maximal margin classifiers. Ie. they seek to find the decision boundary in which the margin between a data point and the decision boundary correctly classifying that datapoint is maximized. Since the decision boundary, is being maximized on both sides (positive/negative) class, SVM's are optimized to generate a decision boundary that maximizes the distance both classes are from the decision boundary, ultimately linearly seperating the data to the best of its ability.\n",
    "* The primary hyperparameter for a SVM is the soft-margin constant (C), which operates as the size of the penalty assigned to errors/margin errors. Decreasing or Increasing this value, ultimately changes the size of the acceptable margin on each side of the decision boundary, often shifting the boundary itself, as the boundary accomodates erroenous or more difficult to fit data.\n",
    "* The secondary hyperparameter decision primarily surrounds the kernel. Changing the polynomial of the kernel, or changing the kernel function alltogether introduces additional non-linearity and allows the decision boundary to accomodate for non-linearly seperable data.\n",
    "* The common workflow for determining SVM's hyperparameters and kernel functions is as follows: Try a linear kernel first, and then see if we can improve on its performance using a non-linear kernel. The linear kernel oftens provides a valuable baseline, and high polynomial kernels or gaussian kernels can lead to overfitting on data with high dimensionality and a small number of examples.\n",
    "* Large margin classifiers are known to be sensitive to the way features are scaled. Therefore it is essential to normalize either the data or the kernel itself.\n",
    "* Interesting note on standardization: \"Standardization is not appropriate when the data is sparse since it destroys sparsity since each feature will typically have a different normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [ESL: Support Vector Machines](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Notes on Support Vector Machines, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space.\n",
    "* Primarily, linearly Seperable classifiers, maximize the margin between data points and the decision boundary. However, often in reality all points are not linearly seperable. One way to deal with this overlap is still to maximize the margin, but allow for some points to be on the wrong side of the margin. This method measures the overlap in relative distance from the margin, which changes with the width of the margin M. This choise is a convex optimization problem (quadratic criterion, linear inequality constraints), which is the \"standard\" support vector machine.\n",
    "* Points well inside their class boundary do not play a big role in shaping the boundary.\n",
    "* Discuss the cost parameter (C): Hence larger values of C focus atention more on (correctly classified) points near the decision boundary, while smaller values involve data further away. Either way, misclassified points are given weight, no matter how far away.\n",
    "* The optimal value for C can be estimated by cross-validation. Interestingly, the leave-one-out cross-validation error can be bounded above by the proportion of support points in the data. The reason is that leaving out an observation that is not a support vector will not change the solution. Hence these observations, being classified correctly by the original boundary, will be classified correctly in the cross-validation process. However this bound tends to be too high, and not generally useful for choosing C.\n",
    "* What are support vectors? Points on the wrong side of the boundary are support vectors. In addition, points on the correct side of the boundary but close to it (in the margin), are also support vectors.\n",
    "* Frame Non-Linear SVMs, as a linear boundary in an enlarged feature space, in which the feature space is enlarged via basis expansions such as polynomials or splines. Generally, linear boundaries in the enlarged space achieve better training-class seperation, and translate to nonlinear boundaries in the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [ESL: K-Nearest Neighbours](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Memory based classifier, which requires no model to be fit. Given a query point x, we find the k training points closest in distance to x, and then classify using majority vote among the k neighbours.\n",
    "* Referred to as a Prototype method. Prototype methods represent the training data by a set of points in feature space. These prototypes are typically note examples from the training sample, except in the case of 1-nearest-neighbour classification. Each prototype has an associated class label, and classification of a query point (x) is made to the class of the closest prototype. \"Closes\" is usually defined by Euclidean distance in the feature space, after each feature has been standardized to have overall mean of 0 and variance 1 in the training sample.\n",
    "* Despite its simplicity, K-Nearest Neighbours is often successful where each class has many possible prototypes, and the decision boundary is very irregular.\n",
    "* Often works fairly well for lower-dimensional problems, when nearest neighbour classification is carried out in a high-dimensional feature space, the nearest neighbours of a point can be very far away, causing bias and degrading the performance of the rule.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [ESL: K-Means Clustering](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "K-Means Algorithm is one of the most popular iterative descent clustering methods.\n",
    "* It is intended for situations in which all variables are of the quantitative type, and squared Euclidean distance is chosen as the dissimilarity measure.\n",
    "* Thus, the criterion is minimized by assigning the N observations to the K clusters in such a way that within each cluster the average dissimiliarity of the observations from the cluster mean, as defined by the points in that cluster, is minimized.\n",
    "\n",
    "* It can be solved in the following fashion:\n",
    "    * Create k random cluster centroids, for your particular euclidean space.\n",
    "    * Assign each point in the dataset to the closest cluster centroid.\n",
    "    * Calculate the mean of the points inside each cluster assignment, and set the cluster centroid to that mean value.\n",
    "    * Repeat steps 2/3 until the cluster assignments do not change.   \n",
    "    \n",
    "    \n",
    "* It is also best practice, to start the algorithm with many different random choices for the starting means, and choose the solution having the smallest value of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [Nonnegative Matrix Factorization for Dummies](http://www.billconnelly.net/?p=534)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A decomposition method to identify the underlying drivers of a signal or value.\n",
    "* NMF only works on signals that are always positive.\n",
    "* Factorization is an action in linear algebra to decompose a matrix, into two generative matrices. These generative matrices when multiplied together, should generate the original target matrix.\n",
    "* NMF can be optimized against a loss function in order, to decompose or generate factors out of raw data into a specific dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model Assessment and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [ESL: Bias, Variance and Model Complexity](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Notes on generalization and the Bias/Variance tradeoff along with ideas on splitting data. \n",
    "* The *generalization* performance of a learning method relates to its prediction capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us a measure of the quality of the ultimately chosen model.  \n",
    "* Test error, also referred to as *generalization error*, is the prediction error over an independent test sample. Whereas training error is the average loss over a training sample.  \n",
    "* As the model becomes more and more complex, it uses the training data more and is able to adapt to more complicated underlying structures. Hence there is a decrease in bias but an increase in variance. There is some intermediate model complexity that gives minimum expected test error.\n",
    "* Training error consistently decreases with model complexity, typically dropping to zero if we increase the model complexity enough. However, a model with zero training error is overfit to the training data and will typically generalize poorly.  \n",
    "* In a data-rich situation, the best approach for both model selection and assessment is to randomly divide the dataset into three parts: a training set, a validation set, and a test set. The training set is used to fit the models; the validation set is used to estimate prediction error for model selection; the test set is used for assessment of the generalization error of the final chosen model.\n",
    "* For a regression fit's redducible error, there are primarily two components bias and variance. Bias, is the amount by which the average of our estimate differs from the true mean. The variance, is the expected squared deviation of our learned function around its mean. Typically, the more complex we make the model, the lower the (squared) bias but the higher the variance.\n",
    "* In other words, bias are the simplifying assumptions made by a model to make the target function easier to learn. Whereas variance is the amount that the estimate of the target function will change if different training data is used. They are also inversely proportional, ie. Increasing the bias will decrease the variance and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [ESL: Cross Validation](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ideally, if we had enough data, we would set aside a validation set and use it to assess the performance of our prediction model. Since data are often scarce, this is usually not possible. To finesse the problem, K-fold cross-validation uses part of the available data to fit the model, and a different part to test it.\n",
    "* We split the data into K roughly equal-sized parts. For the kth part, we fit the model to the other K-1 parts of the data, and calculate the prediction error of the fitted model when predicting the kth part of the data. We do this for k = 1,2, ... , K and combine the K estimates of prediction error.\n",
    "* We use this method to choose our model, which is then fit on all available training data.\n",
    "* Cross-Validation only estimates effectively the average error Err. As K approaches N, the cross-validation estimator is approximately unbiased for the true (expected) prediction error, but can have high variance because the N \"training sets\" are so similar to one another. The computational burden is also considerable, requiring N applications of the learning method. On the other hand, with K=5 or example, cross-validation has lower variance. But bias could be a problem, depending on how the performance of the learning method varies with the size of the training set.\n",
    "* To summarize, if the learning curve has a considerable slope at the given training set size, (accuracy increases quickly as a result of training set size), five or tenfold cross-validation will overestimate the true prediction error.\n",
    "* In general, with a multistep modeling procedure, cross-validation must be applied to the entire sequence of modeling steps. In particular, samples must be \"left out\" before any selection or filtering steps are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
