{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Traditional Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [A User's Guide to Support Vector Machines](http://pyml.sourceforge.net/doc/howto.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Support Vector Machine (SVM) is a widely used classifier. We describe the effect of the SVM parameters on the resulting classifier, how to select good values for those parameters, data normalization, factors that affect training time.\n",
    "* SVMs belong to the general category of kernel methods. A kernel method is an algorithm that depends on the data only through dot-products. When this is the case, the dot product can be replaced by a kernel function which computes a dot product in some possibly high dimensional feature space. This has two advantages: First, the ability to generate non-linear decision boundaries using methods designed for linear classifiers. Second, the use of kernel funcions allows the user to apply a classifier to data that have no obvious fixed-dimensional vector space representation.\n",
    "* The basic support vector machine is a linear two-class classifier. In which a linear decision boundary defined by a hyperplane classifies the input space into positive/negative classes.\n",
    "* Linear SVMs can be extended by applying non-linear mapping functions, to generate non-linear decision boundaries. However, the approach of explicitly computing non-linear features via a specifically created non-linear mapping of the features, does not scale well with the number of input features. Kernel methods solve this issue by avoiding the step of explicily mapping the data to a high dimensional feature-space.\n",
    "* Traditionally, SVM's are solved as maximal margin classifiers. Ie. they seek to find the decision boundary in which the margin between a data point and the decision boundary correctly classifying that datapoint is maximized. Since the decision boundary, is being maximized on both sides (positive/negative) class, SVM's are optimized to generate a decision boundary that maximizes the distance both classes are from the decision boundary, ultimately linearly seperating the data to the best of its ability.\n",
    "* The primary hyperparameter for a SVM is the soft-margin constant (C), which operates as the size of the penalty assigned to errors/margin errors. Decreasing or Increasing this value, ultimately changes the size of the acceptable margin on each side of the decision boundary, often shifting the boundary itself, as the boundary accomodates erroenous or more difficult to fit data.\n",
    "* The secondary hyperparameter decision primarily surrounds the kernel. Changing the polynomial of the kernel, or changing the kernel function alltogether introduces additional non-linearity and allows the decision boundary to accomodate for non-linearly seperable data.\n",
    "* The common workflow for determining SVM's hyperparameters and kernel functions is as follows: Try a linear kernel first, and then see if we can improve on its performance using a non-linear kernel. The linear kernel oftens provides a valuable baseline, and high polynomial kernels or gaussian kernels can lead to overfitting on data with high dimensionality and a small number of examples.\n",
    "* Large margin classifiers are known to be sensitive to the way features are scaled. Therefore it is essential to normalize either the data or the kernel itself.\n",
    "* Interesting note on standardization: \"Standardization is not appropriate when the data is sparse since it destroys sparsity since each feature will typically have a different normalization constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### [ESL: Support Vector Machines](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Notes on Support Vector Machines, which produces nonlinear boundaries by constructing a linear boundary in a large, transformed version of the feature space.\n",
    "* Primarily, linearly Seperable classifiers, maximize the margin between data points and the decision boundary. However, often in reality all points are not linearly seperable. One way to deal with this overlap is still to maximize the margin, but allow for some points to be on the wrong side of the margin. This method measures the overlap in relative distance from the margin, which changes with the width of the margin M. This choise is a convex optimization problem (quadratic criterion, linear inequality constraints), which is the \"standard\" support vector machine.\n",
    "* Points well inside their class boundary do not play a big role in shaping the boundary.\n",
    "* Discuss the cost parameter (C): Hence larger values of C focus atention more on (correctly classified) points near the decision boundary, while smaller values involve data further away. Either way, misclassified points are given weight, no matter how far away.\n",
    "* The optimal value for C can be estimated by cross-validation. Interestingly, the leave-one-out cross-validation error can be bounded above by the proportion of support points in the data. The reason is that leaving out an observation that is not a support vector will not change the solution. Hence these observations, being classified correctly by the original boundary, will be classified correctly in the cross-validation process. However this bound tends to be too high, and not generally useful for choosing C.\n",
    "* What are support vectors? Points on the wrong side of the boundary are support vectors. In addition, points on the correct side of the boundary but close to it (in the margin), are also support vectors.\n",
    "* Frame Non-Linear SVMs, as a linear boundary in an enlarged feature space, in which the feature space is enlarged via basis expansions such as polynomials or splines. Generally, linear boundaries in the enlarged space achieve better training-class seperation, and translate to nonlinear boundaries in the original space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
